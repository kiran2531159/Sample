from pyspark.sql import SparkSession
import os
import Orange

# Set the KRB5_CONFIG environment variable (optional, if using a custom krb5.conf)
krb5_conf_path = "/path/to/custom/krb5.conf"  # Replace with your krb5.conf path
os.environ["KRB5_CONFIG"] = krb5_conf_path

# Assume a Kerberos ticket is already available in the cache
# You can verify this manually with: klist
principal = "user@REALM.COM"  # Your Kerberos principal

# Check if a ticket exists (optional validation)
try:
    output = subprocess.check_output("klist", shell=True, text=True)
    if principal not in output:
        raise Exception(f"No valid ticket found for {principal}. Run 'kinit {principal}' first.")
    print("Using existing Kerberos ticket")
except subprocess.CalledProcessError as e:
    raise Exception(f"Kerberos ticket check failed: {e}. Ensure you have a valid ticket.")
spark = SparkSession.builder \
    .appName("OrangeKerberosSparkNoKeytab") \
    .master("yarn") \
    .config("spark.yarn.principal", "user@REALM.COM") \
    .config("spark.hadoop.security.authentication", "kerberos") \
    .config("spark.driver.java.options", "-Dsun.security.krb5.debug=true") \  # Enable Kerberos debug
    .getOrCreate()

# Verify Spark session
print(f"Spark version: {spark.version}")

# Load and process data from a Kerberos-secured HDFS
df = spark.read.csv("hdfs://namenode:8021/path/to/data.csv", header=True, inferSchema=True)
filtered_df = df.filter(df["some_column"] > 10)

# Convert to Orange Table
pandas_df = filtered_df.toPandas()
out_data = Orange.data.Table(pandas_df)

# Stop Spark session
spark.stop()